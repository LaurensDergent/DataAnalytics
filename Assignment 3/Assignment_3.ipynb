{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be934e6-e523-4d30-aa7a-dbd12119fb01",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "**Group 5**\n",
    "\n",
    "Group members:\n",
    "- Moqian Chen (r0965473)\n",
    "- Laurens Dergent (r0794288)\n",
    "- Sarah Guilliams (r0751825)\n",
    "- Yeabsera Kinfu (r0930148)\n",
    "- Jorge Puertolas Molina (r0978889)\n",
    "- Isabel Scholz (r1008561)\n",
    "\n",
    "Link to the GitHub repository: https://github.com/LaurensDergent/DataAnalytics.git "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf04691-4714-40b7-b5ea-3d14c8e7ec4c",
   "metadata": {},
   "source": [
    "## 1. Spark setting and data import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5296fe-77da-4ecb-8484-bd5faa0c80f9",
   "metadata": {},
   "source": [
    "The task is about streaming analysis, as well as text analysis, building machine learning models specifically with PySpark streaming context and MLBib. The data collected are from a Spark setting. The gathered data contained 6.6 thousand instances that spreaded over 3 weeks, from the 14th of april to the 12th of May. After duplicate removal there were 5.7 thousand entries in the data set, which is large enough for a mahchine learning model on text analysis. The data was then ordered to ensure that future data wasn't being used to predict past data. The data is then split for training and testing and preprocessed as what should be done normally in machine learning. The label frontpage was transformed to a binary variable, where cases for which frontpage was equal to true were treated as positive, and on the contrary, articles that did not, were treated as negative . Alongside, some text-analysis-specific methods are used: removal of punctuation, stopwords, empty spaces. The models we tried were NaiveBayes and LogisticRegression.\n",
    "\n",
    "In the model experimentation/selection section, there are 2 steps, one where we run the models using TF-IDF, and one where we run the models using Word2Vec and LDA Topic Modelling. This has an explanation. In the beginning, the goal was to encode the source text using TF-IDF. The reasoning behind this was to obtain relationships between word incidence and whether an article will end up on the frontpage. We thought that the inclusion of the IDF factor, would highlight niche words in each article much more, and we would be able to infer useful connections between a word, and the likelihood that the article the word was in would be popular. This assumption was wrong, and the models performed horribly on TF-IDF, especially Naive Bayes. For this reason, we decided to seek solutions in other methods. Latent Dirichtlet Allocation and Word2Vec seemed like good options because they didn't produce highly dimensional data, and captured denser, richer contextual information than TF-IDF. The code for these methods is presented earlier than the use of them. \n",
    "\n",
    "The last section of the assignment focuses on model deployment. Out of all tested models, the best one is stored. Later, in the deployment section, the incoming data is preprocessed and predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e8d30c-adf5-4615-ba55-701129b84f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, ArrayType\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, VectorAssembler, HashingTF, IDF, Word2Vec, Normalizer, MinMaxScaler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.clustering import LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b82eb1-2138-4b2c-9047-f434ea6708ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5a2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.8.19 (default, Mar 20 2024 19:55:45)\n",
      "Spark context Web UI available at http://192.168.1.14:4045\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1716726499182).\n",
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.14:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.shell import sc\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e558ca6-01ae-404d-834c-bb48a4ac0b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.14:4045\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15caa9744c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ce2783-b61f-4efc-9a24-69a9631f7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read through all the subdirectories saved\n",
    "df = spark.read.json(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569975a2-e449-4f47-bdbd-e52552c492bd",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0f6e9-e53c-40f1-9985-019ea1176d26",
   "metadata": {},
   "source": [
    "Several processing steps were needed to prepare the Spark Dataframe for model training. Some columns that were not useful in the prediction task were dropped. For columns like user, aid, url, posted_at and domain, they are of no use because they are too unique. There are two 'titles': 'title' and 'source_title'. Only 'title' was kept considering the fact that people would browse Hector News initially instead of browsing the source titles. Missing values are detected and dropped as well. As inspected, there are two kinds of missing values. One type is 'NULL' contents. Another one is '404 error' (shown as 'Page not found')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc594d7a-492e-4842-ae70-1d96c34175d7",
   "metadata": {},
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7aee8b-2d7d-4ce8-b76c-e1efe13bccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5753"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate rows and count\n",
    "df = df.dropDuplicates()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a727bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------+-------------------+--------------------+\n",
      "|     aid|               title|                 url|frontpage|         source_text|votes|comments|           domain|     user|          posted_at|        source_title|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------+-------------------+--------------------+\n",
      "|40364744|A design for micr...|https://www.nextb...|    false|Millimeter accura...|    1|       0|nextbigfuture.com|    fanf2|2024-05-15 09:24:02|Millimeter accura...|\n",
      "|40133032|Unreal Engine 5.4...|https://dev.epicg...|    false|Unreal Engine 5.4...|    1|       0|    epicgames.com|makepanic|2024-04-23 15:27:19|Unreal Engine 5.4...|\n",
      "|40206221|Sunlight and Vita...|https://www.ncbi....|     true|Sunlight and Vita...|   43|      42|          nih.gov|      luu|2024-04-30 01:10:50|Sunlight and Vita...|\n",
      "|40360417|Bryan Caplan on A...|https://reason.co...|    false|Q&A: Bryan Caplan...|    1|       0|       reason.com| jseliger|2024-05-14 21:35:02|Q&A: Bryan Caplan on|\n",
      "|40269847|Probing single el...|https://www.natur...|    false|Probing single el...|    1|       0|       nature.com|  gnabgib|2024-05-06 00:11:55|Probing single el...|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"aid\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"frontpage\", StringType(), True),\n",
    "    StructField(\"source_text\", StringType(), True),\n",
    "        StructField(\"votes\", IntegerType(), True),\n",
    "    StructField(\"comments\", IntegerType(), True),\n",
    "\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"user\", StringType(), True),\n",
    "    StructField(\"posted_at\", StringType(), True),\n",
    "        StructField(\"source_title\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Parse the JSON strings using the defined schema\n",
    "df = df.withColumn(\"parsed_value\", from_json(col(\"value\"), schema)).select(\"parsed_value.*\")\n",
    "\n",
    "# Show the DataFrame with all columns\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda44cbb-42f2-4777-b00f-21405e934d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('frontpage', when(df.frontpage==True, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae6acfe-c712-4b45-8597-c7b4d3d2490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+--------------------+-------------+-------------------+--------------------+\n",
      "|     aid|               title|                 url|frontpage|         source_text|votes|comments|              domain|         user|          posted_at|        source_title|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+--------------------+-------------+-------------------+--------------------+\n",
      "|40364744|A design for micr...|https://www.nextb...|        0|Millimeter accura...|    1|       0|   nextbigfuture.com|        fanf2|2024-05-15 09:24:02|Millimeter accura...|\n",
      "|40133032|Unreal Engine 5.4...|https://dev.epicg...|        0|Unreal Engine 5.4...|    1|       0|       epicgames.com|    makepanic|2024-04-23 15:27:19|Unreal Engine 5.4...|\n",
      "|40206221|Sunlight and Vita...|https://www.ncbi....|        1|Sunlight and Vita...|   43|      42|             nih.gov|          luu|2024-04-30 01:10:50|Sunlight and Vita...|\n",
      "|40360417|Bryan Caplan on A...|https://reason.co...|        0|Q&A: Bryan Caplan...|    1|       0|          reason.com|     jseliger|2024-05-14 21:35:02|Q&A: Bryan Caplan on|\n",
      "|40269847|Probing single el...|https://www.natur...|        0|Probing single el...|    1|       0|          nature.com|      gnabgib|2024-05-06 00:11:55|Probing single el...|\n",
      "|40231369|Top Steam Tags by...|https://games-sta...|        0|Top Steam Tags by...|    1|       0|     games-stats.com| beckthompson|2024-05-02 00:29:59|Top Steam Tags by...|\n",
      "|40351788|Show HN: Advanced...|https://www.micpr...|        0|Mic Prices | Find...|    1|       0|       micprices.net|       tomclr|2024-05-14 05:06:12|Mic Prices | Find...|\n",
      "|40179918|Saudi Arabia Spen...|https://www.nytim...|        0|â€˜To the Futureâ€™: ...|    2|       0|         nytimes.com|    bookofjoe|2024-04-27 13:47:06|â€˜To the Futureâ€™: ...|\n",
      "|40210735|Watchdog reveals ...|https://www.there...|        0|Watchdog reveals ...|    2|       0|     theregister.com|  LinuxBender|2024-04-30 13:25:20|Watchdog reveals ...|\n",
      "|40338039|             Wallpea| https://wallpea.com|        0|Wallpea\\n\\n  * ME...|    1|       0|         wallpea.com|     Folzitan|2024-05-12 22:25:56|             Wallpea|\n",
      "|40131022|Virtual reality-e...|https://www.natur...|        0|Virtual reality-e...|    2|       0|          nature.com|    bookofjoe|2024-04-23 12:13:39|Virtual reality-e...|\n",
      "|40329155|A cyberattack hit...|https://securitya...|        0|A cyberattack hit...|    1|       0| securityaffairs.com|       ssahoo|2024-05-11 16:40:26|A cyberattack hit...|\n",
      "|40363971|      Notes on Japan|https://alexander...|        1|Notes on Japan\\n\\...|    9|       2|alexanderweichart.de|     surrTurr|2024-05-15 07:10:38|      Notes on Japan|\n",
      "|40361737|Scientists demyst...|https://medicalxp...|        0|Scientists demyst...|    1|       0|   medicalxpress.com|         wglb|2024-05-15 00:32:57|Scientists demyst...|\n",
      "|40179841|Philosopher Nick ...|https://bigthink....|        0|Philosopher Nick ...|    1|       0|        bigthink.com|   Brajeshwar|2024-04-27 13:33:15|Philosopher Nick ...|\n",
      "|40108737|The Evolution of ...|https://www.ncbi....|        0|The evolution of ...|    2|       0|             nih.gov|    andsoitis|2024-04-21 20:02:00|The evolution of ...|\n",
      "|40232053|Patients with rhe...|https://medicalxp...|        0|Patients with rhe...|    2|       0|   medicalxpress.com|         wglb|2024-05-02 02:15:24|Patients with rhe...|\n",
      "|40039887|Specific nasal ce...|https://medicalxp...|        0|Specific nasal ce...|    2|       0|   medicalxpress.com|    pseudolus|2024-04-15 12:48:02|Specific nasal ce...|\n",
      "|40170886|How Much Is Enoug...|https://lithub.co...|        0|How Much is Enoug...|    1|       0|          lithub.com|    PaulHoule|2024-04-26 15:45:39|How Much is Enoug...|\n",
      "|40039679|Tesla lays off 'm...|https://electrek....|        0|Tesla lays off 'm...|    2|       0|         electrek.co|keyboardJones|2024-04-15 12:27:08|Tesla lays off 'm...|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+--------------------+-------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39476fe-0b63-4e36-85fc-c1c8a31cf3bd",
   "metadata": {},
   "source": [
    "### Order Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e3b3fc-be33-448f-a647-1f7d95a1beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Time in ascending order\n",
    "df_sorted = df.orderBy(\"posted_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b97d09-e548-419e-a837-744d51cf0957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------------+-------------------+--------------------+\n",
      "|     aid|               title|                 url|frontpage|         source_text|votes|comments|           domain|           user|          posted_at|        source_title|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------------+-------------------+--------------------+\n",
      "|40038747|Le RÃ´le de l'IA d...|https://www.rubyb...|        0|[HORS SÃ‰RIE #6] T...|    1|       0|   rubybiscuit.fr|         dom_fr|2024-04-15 10:39:26|[HORS SÃ‰RIE #6] T...|\n",
      "|40038755|After delay due t...|https://www.there...|        0|Ubuntu 24.04 'Nob...|    1|       0|  theregister.com|        lproven|2024-04-15 10:40:57|Ubuntu 24.04 'Nob...|\n",
      "|40038759|Weird monitor bug...|https://notes.ali...|        0|Weird monitor bug...|    1|       0| alinpanaitiu.com|          fanf2|2024-04-15 10:42:04|Weird monitor bug...|\n",
      "|40038860|SQL Optimizations...|https://www.perco...|        0|SQL Optimizations...|    2|       0|      percona.com|         tie-in|2024-04-15 10:56:16|SQL Optimizations...|\n",
      "|40038882|Marques Brownlee ...|https://www.youtu...|        0|The Worst Product...|    1|       0|      youtube.com|         latexr|2024-04-15 10:59:33|The Worst Product...|\n",
      "|40038885|The security impl...|https://martijnho...|        0|The security impl...|    2|       0|   martijnhols.nl|    MartijnHols|2024-04-15 10:59:52|The security impl...|\n",
      "|40038888|Neverest, a CLI t...|https://github.co...|        0|GitHub - soywod/n...|    1|       0|github.com/soywod|   todsacerdoti|2024-04-15 11:00:06|GitHub - soywod/n...|\n",
      "|40038890|Filesystem Error ...|http://danluu.com...|        0|Filesystem error ...|    2|       0|       danluu.com|herecomethefuzz|2024-04-15 11:00:16|Filesystem error ...|\n",
      "|40038914|OpenRazer 3.8 Bri...|https://www.phoro...|        0|OpenRazer 3.8 Bri...|    2|       0|     phoronix.com|        rbanffy|2024-04-15 11:03:34|OpenRazer 3.8 Bri...|\n",
      "|40038919|Terminal based se...|https://github.co...|        0|GitHub - sjurba/r...|    2|       0|github.com/sjurba|    alexzeitler|2024-04-15 11:04:23|GitHub - sjurba/r...|\n",
      "|40038921|How Perfectly Can...|https://www.newyo...|        0|How Perfectly Can...|    2|       0|    newyorker.com|      adrianhon|2024-04-15 11:05:02|How Perfectly Can...|\n",
      "|40038922|Are Flying Cars H...|https://www.newyo...|        1|Are Flying Cars F...|    8|       6|    newyorker.com|      adrianhon|2024-04-15 11:05:27|Are Flying Cars F...|\n",
      "|40038929|GCC version 14 co...|https://www.there...|        0|GCC version 14 co...|    1|       0|  theregister.com|        rbanffy|2024-04-15 11:05:38|GCC version 14 co...|\n",
      "|40038934|Malleable Softwar...|https://www.geoff...|        0|Malleable softwar...|    2|       0| geoffreylitt.com|        r_singh|2024-04-15 11:06:41|Malleable softwar...|\n",
      "|40038950|Research: America...|https://techxplor...|        0|Research finds Am...|    2|       0|   techxplore.com|         rustoo|2024-04-15 11:08:27|Research finds Am...|\n",
      "|40038963|      Cammy/Bikeshed|https://esolangs....|        0|Cammy/Bikeshed - ...|    1|       0|     esolangs.org|082349872349872|2024-04-15 11:10:00|Cammy/Bikeshed - ...|\n",
      "|40038974|Rvbbit: Reactive ...|https://www.rvbbi...|        0|RVBBIT\\n\\nðŸ”¥ Firs...|    1|       0|       rvbbit.com|           tosh|2024-04-15 11:11:07|              RVBBIT|\n",
      "|40039003|Argentina's presi...|https://apnews.co...|        0|Argentina's Javie...|    1|       0|       apnews.com|           geox|2024-04-15 11:15:18|Argentina's popul...|\n",
      "|40039010|Verse: A New Func...|https://www.youtu...|        0|Verse: A New Func...|    1|       0|      youtube.com|    thunderbong|2024-04-15 11:16:20|Verse: A New Func...|\n",
      "|40039020|Python is Actuall...|https://ahgamut.g...|        1|Python is Actuall...|   21|       0|ahgamut.github.io|    thunderbong|2024-04-15 11:17:44|Python is Actuall...|\n",
      "+--------+--------------------+--------------------+---------+--------------------+-----+--------+-----------------+---------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190707d2-75c2-41c1-8e31-0ea695c16c4a",
   "metadata": {},
   "source": [
    "### Select necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae2a15b-b6b5-4410-8450-742295bf7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns ('source_title', 'frontpage', 'comments', 'votes', 'domain', 'posted_at') and show the transformed dataframe for check\n",
    "\n",
    "df_sorted = df_sorted.select(col('comments'), col('frontpage'), col('source_text'), col('votes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc994cd-008f-427f-9dde-217d131f25fd",
   "metadata": {},
   "source": [
    "### Removing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084d3f29-76b8-4de9-8202-da34a73e974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5696"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values check: 2 types could be viewed as missing values, then count\n",
    "# Type 1: Page not found\n",
    "# Type 2: NULL\n",
    "df_sorted = df_sorted.where(df.source_text != 'Page not found')\n",
    "df_sorted.dropna()\n",
    "df_sorted.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8af23-bf6a-4d9c-9f40-a038b5363d5b",
   "metadata": {},
   "source": [
    "### Encoding the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa49ccc-7bb6-4130-9e43-fd01bbcea521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+-----+\n",
      "|comments|frontpage|         source_text|votes|\n",
      "+--------+---------+--------------------+-----+\n",
      "|       0|        0|[HORS SÃ‰RIE #6] T...|    1|\n",
      "|       0|        0|Ubuntu 24.04 'Nob...|    1|\n",
      "|       0|        0|Weird monitor bug...|    1|\n",
      "|       0|        0|SQL Optimizations...|    2|\n",
      "|       0|        0|The Worst Product...|    1|\n",
      "|       0|        0|The security impl...|    2|\n",
      "|       0|        0|GitHub - soywod/n...|    1|\n",
      "|       0|        0|Filesystem error ...|    2|\n",
      "|       0|        0|OpenRazer 3.8 Bri...|    2|\n",
      "|       0|        0|GitHub - sjurba/r...|    2|\n",
      "|       0|        0|How Perfectly Can...|    2|\n",
      "|       6|        1|Are Flying Cars F...|    8|\n",
      "|       0|        0|GCC version 14 co...|    1|\n",
      "|       0|        0|Malleable softwar...|    2|\n",
      "|       0|        0|Research finds Am...|    2|\n",
      "|       0|        0|Cammy/Bikeshed - ...|    1|\n",
      "|       0|        0|RVBBIT\\n\\nðŸ”¥ Firs...|    1|\n",
      "|       0|        0|Argentina's Javie...|    1|\n",
      "|       0|        0|Verse: A New Func...|    1|\n",
      "|       0|        1|Python is Actuall...|   21|\n",
      "+--------+---------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode the label column 'frontpage' and show it to verify\n",
    "df_sorted = df_sorted.withColumn('frontpage', when(df.frontpage==True, 1).otherwise(0))\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a06e5-0268-4249-9959-a99295c1e597",
   "metadata": {},
   "source": [
    "### Removing punctuations, stopwords and tokenizing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f1e0f",
   "metadata": {},
   "source": [
    "In preparation of featurization, the 'source_text' variable is tokenized so that sentences could be sliced into single words. After tokenization, punctuations and stop words have been removed. Then all the tokens are stemmed in terms of syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08884ed8-19a4-4099-ab92-a0fd90675fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text, remove the punctuations ('/\"/,/./:/-/?/!/:/|/[/])\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_punc_drop = df_sorted.withColumn('source_text', regexp_replace(df.source_text, '[^a-zA-Z0-9]', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04a3aa0c-3d10-4a37-9dfa-227f0018ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+-----+--------------------+\n",
      "|comments|frontpage|         source_text|votes|              tokens|\n",
      "+--------+---------+--------------------+-----+--------------------+\n",
      "|       0|        0| HORS S RIE  6  T...|    1|[, hors, s, rie, ...|\n",
      "|       0|        0|Ubuntu 24 04  Nob...|    1|[ubuntu, 24, 04, ...|\n",
      "|       0|        0|Weird monitor bug...|    1|[weird, monitor, ...|\n",
      "|       0|        0|SQL Optimizations...|    2|[sql, optimizatio...|\n",
      "|       0|        0|The Worst Product...|    1|[the, worst, prod...|\n",
      "|       0|        0|The security impl...|    2|[the, security, i...|\n",
      "|       0|        0|GitHub   soywod n...|    1|[github, , , soyw...|\n",
      "|       0|        0|Filesystem error ...|    2|[filesystem, erro...|\n",
      "|       0|        0|OpenRazer 3 8 Bri...|    2|[openrazer, 3, 8,...|\n",
      "|       0|        0|GitHub   sjurba r...|    2|[github, , , sjur...|\n",
      "|       0|        0|How Perfectly Can...|    2|[how, perfectly, ...|\n",
      "|       6|        1|Are Flying Cars F...|    8|[are, flying, car...|\n",
      "|       0|        0|GCC version 14 co...|    1|[gcc, version, 14...|\n",
      "|       0|        0|Malleable softwar...|    2|[malleable, softw...|\n",
      "|       0|        0|Research finds Am...|    2|[research, finds,...|\n",
      "|       0|        0|Cammy Bikeshed   ...|    1|[cammy, bikeshed,...|\n",
      "|       0|        0|RVBBIT    First R...|    1|[rvbbit, , , , fi...|\n",
      "|       0|        0|Argentina s Javie...|    1|[argentina, s, ja...|\n",
      "|       0|        0|Verse  A New Func...|    1|[verse, , a, new,...|\n",
      "|       0|        1|Python is Actuall...|   21|[python, is, actu...|\n",
      "+--------+---------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For text, make every word in lowercase\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "df_token = Tokenizer(inputCol=\"source_text\", outputCol=\"tokens\").transform(df_punc_drop)\n",
    "df_token.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57d95d27-b619-4659-a95e-245cf9ac0d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text, remove stop words (a/an/the/then/and...)\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopwords = StopWordsRemover()\n",
    "stopwords.getStopWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06fa298d-aa32-457b-9156-2303301b03a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+-----+--------------------+--------------------+\n",
      "|comments|frontpage|         source_text|votes|              tokens|               words|\n",
      "+--------+---------+--------------------+-----+--------------------+--------------------+\n",
      "|       0|        0| HORS S RIE  6  T...|    1|[, hors, s, rie, ...|[, hors, rie, , 6...|\n",
      "|       0|        0|Ubuntu 24 04  Nob...|    1|[ubuntu, 24, 04, ...|[ubuntu, 24, 04, ...|\n",
      "|       0|        0|Weird monitor bug...|    1|[weird, monitor, ...|[weird, monitor, ...|\n",
      "|       0|        0|SQL Optimizations...|    2|[sql, optimizatio...|[sql, optimizatio...|\n",
      "|       0|        0|The Worst Product...|    1|[the, worst, prod...|[worst, product, ...|\n",
      "|       0|        0|The security impl...|    2|[the, security, i...|[security, implic...|\n",
      "|       0|        0|GitHub   soywod n...|    1|[github, , , soyw...|[github, , , soyw...|\n",
      "|       0|        0|Filesystem error ...|    2|[filesystem, erro...|[filesystem, erro...|\n",
      "|       0|        0|OpenRazer 3 8 Bri...|    2|[openrazer, 3, 8,...|[openrazer, 3, 8,...|\n",
      "|       0|        0|GitHub   sjurba r...|    2|[github, , , sjur...|[github, , , sjur...|\n",
      "|       0|        0|How Perfectly Can...|    2|[how, perfectly, ...|[perfectly, reali...|\n",
      "|       6|        1|Are Flying Cars F...|    8|[are, flying, car...|[flying, cars, fi...|\n",
      "|       0|        0|GCC version 14 co...|    1|[gcc, version, 14...|[gcc, version, 14...|\n",
      "|       0|        0|Malleable softwar...|    2|[malleable, softw...|[malleable, softw...|\n",
      "|       0|        0|Research finds Am...|    2|[research, finds,...|[research, finds,...|\n",
      "|       0|        0|Cammy Bikeshed   ...|    1|[cammy, bikeshed,...|[cammy, bikeshed,...|\n",
      "|       0|        0|RVBBIT    First R...|    1|[rvbbit, , , , fi...|[rvbbit, , , , fi...|\n",
      "|       0|        0|Argentina s Javie...|    1|[argentina, s, ja...|[argentina, javie...|\n",
      "|       0|        0|Verse  A New Func...|    1|[verse, , a, new,...|[verse, , new, fu...|\n",
      "|       0|        1|Python is Actuall...|   21|[python, is, actu...|[python, actually...|\n",
      "+--------+---------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.setInputCol('tokens').setOutputCol('words')\n",
    "df_clean = stopwords.transform(df_token)\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69db766-e4a8-4d1d-b089-34e7f06eb1be",
   "metadata": {},
   "source": [
    "## More processing: Feature Engineering and Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e026418-9b29-44e3-ae2c-1c4e767862f8",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a80708-9280-4ab5-853d-618ac8c33a14",
   "metadata": {},
   "source": [
    "Comments and votes needed to be normalized for the logistic regression\n",
    "model. Without normalization, the logistic regression was always predicting a 0 (i.e., that the story does not end on the frontpage). This could be explained by the fact that the extreme values were setting the threshold for 1 (i.e., that the story ends on the frontpage) very high due to the low coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba31d05d-e7b0-468a-be91-5755eaf1cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(df, inputCols):\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=inputCols,\n",
    "        outputCol=\"comvot\"\n",
    "    )\n",
    "\n",
    "    assembled_df = assembler.transform(df)\n",
    "    # Initialize Normalizer\n",
    "    normalizer = Normalizer(inputCol=\"comvot\", outputCol=\"norm_comvot\", p=2.0)\n",
    "\n",
    "    # Normalize the data\n",
    "    df = normalizer.transform(assembled_df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b0eb3-80b2-4b56-9d84-8d86ff42a11a",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdfe4fcd-98a7-4bb9-b393-f9491e239040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of negative cases: 4671\n",
      "Count of positive cases: 1025\n"
     ]
    }
   ],
   "source": [
    "print('Count of negative cases:', df_clean.select('frontpage').where(df_clean.frontpage==0).count())\n",
    "print('Count of positive cases:', df_clean.select('frontpage').where(df_clean.frontpage==1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e63080-1d92-4e41-a74b-422579558327",
   "metadata": {},
   "source": [
    "Positive cases are so few that models could be unable to fully learn the patterns, the ratio of negative to positive cases is 4.5. To solve class imbalance, oversampling was used. The method used for oversampling is sampling with Replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "761e5eb9-45a7-4057-bf54-3c10e10ab1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_minority(df, ratio=1):\n",
    "    '''\n",
    "    ratio is the ratio of majority to minority\n",
    "    Eg. ratio 1 is equivalent to majority:minority = 1:1\n",
    "    ratio 5 is equivalent to majority:minority = 5:1\n",
    "    '''\n",
    "    minority_count = df.filter(\"frontpage=1\").count()\n",
    "    majority_count = df.filter(\"frontpage=0\").count()\n",
    "    \n",
    "    balance_ratio = majority_count / minority_count\n",
    "    \n",
    "    print(f\"Initial Majority:Minority ratio is {balance_ratio:.2f}:1\")\n",
    "    if ratio >= balance_ratio:\n",
    "        print(\"No oversampling of minority was done as the input ratio was more than or equal to the initial ratio.\")\n",
    "    else:\n",
    "        print(f\"Oversampling of minority done such that Majority:Minority ratio is {ratio}:1\")\n",
    "    \n",
    "    oversampled_minority = df.filter(\"frontpage=1\")\\\n",
    "                                .sample(withReplacement=True, fraction=(balance_ratio/ratio),seed=88)\n",
    "    oversampled_df = df.filter(\"frontpage=0\").union(oversampled_minority)\n",
    "    \n",
    "    return oversampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616fea16-ac0c-4646-9701-0dfaee55191a",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0f2c29c-a6c4-46dd-985c-6c277f853ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(df, colwords):\n",
    "# Step 1: Compute TF\n",
    "    hashing_tf = HashingTF(inputCol= colwords, outputCol=\"tf_features\", numFeatures=custom_params[\"tf_idf\"])\n",
    "    tf_data = hashing_tf.transform(df)\n",
    "\n",
    "    # Step 2: Compute IDF\n",
    "    idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "    idf_model = idf.fit(tf_data)\n",
    "    df = idf_model.transform(tf_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4e8fb-0ba0-49f5-9e18-5f40ccbbf546",
   "metadata": {},
   "source": [
    "### LDA (LATENT DIRICHTLET ALLOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ac7f0-96a5-40e1-bfb1-f5c0fa5531e0",
   "metadata": {},
   "source": [
    "LDA is another appraoch to reduce dimensions of the feature column. The reduction is similar with what word2vec does. However, instead of considering context, LDA considers topics. It assumes there are some common topics in the corpus. And the vector of each document would indicate how far it is from a certain latent topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dcef78a-a882-4487-8a10-650c9348ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_met(train, colwords, ntopics):\n",
    "    vectorizer = CountVectorizer(inputCol=colwords, outputCol=\"counts\")\n",
    "    vectorizer_model = vectorizer.fit(train)\n",
    "    bow_df = vectorizer_model.transform(train_processed)\n",
    "\n",
    "    # Train the LDA model\n",
    "    num_topics = ntopics \n",
    "    lda = LDA(k=num_topics, maxIter=10, featuresCol=\"counts\")\n",
    "    lda_model = lda.fit(bow_df)\n",
    "    train = lda_model.transform(bow_df)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3253eb-3914-45a1-b9bc-ec67964364fb",
   "metadata": {},
   "source": [
    "### WORD2VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4172cac-6733-48b7-a921-80404db6754d",
   "metadata": {},
   "source": [
    "Word2Vec creates a vector space for the corpus. The size of the vector specifies how many axies the space has. Each word would get a score vector with one score on one axis. The vector of the document, in the end, is one with average values of all the vectors for single words. Then, any document from the corpus would get a coordinate in the space, represented by the vector. In the end, the feature vectors would merely contain much less features without losing any information. Besides, since it is a coordinate, relationships between single words are considered. This attribute complements an obvious shortcoming of TF-IDF other than high dimensionality: context of a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f741d58-3bca-4a24-9244-5ab44804aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vecproc(train, colwords, vecsize):\n",
    "    word2Vec = Word2Vec(vectorSize=vecsize, minCount=0, inputCol=colwords, outputCol=\"w2vfeatures\")\n",
    "    word2vec_model = word2Vec.fit(train)\n",
    "    train = word2vec_model.transform(train)\n",
    "\n",
    "    rescaler = MinMaxScaler(min = 0, max = 1, inputCol = 'w2vfeatures', outputCol='w2vfeatures_rescaled')\n",
    "    rescale = rescaler.fit(train)\n",
    "    train = rescale.transform(train)\n",
    "    return train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce7ce6-30b7-4ff1-bce2-30a46d4edf1c",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997174b-6d0d-4e53-8cb6-dd761b377429",
   "metadata": {},
   "source": [
    "The train/test data split is done in chronological order, so that the first 75 % of the data is the train set, and the last 25% is the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a655a92e-b3a1-40a2-bb89-ccc0c287710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintestsplit(df_final):\n",
    "    # Select relevant columns for the model\n",
    "    df_final = df_final.select(\"features\", \"frontpage\")\n",
    "\n",
    "    # Count the total number of rows\n",
    "    total_count = df_final.count()\n",
    "    train_count = int(total_count * 0.75)\n",
    "    test_count = total_count - train_count\n",
    "\n",
    "    # Select the first 80% of the rows for the training set\n",
    "    train = df_final.limit(train_count)\n",
    "\n",
    "    # Select the remaining 20% of the rows for the test set\n",
    "    test = df_final.subtract(train).limit(test_count)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714365d-e2cf-4080-acf1-7535c9e5829e",
   "metadata": {},
   "source": [
    "## MODEL EXPERIMENTATION/SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cebdd-4c93-4ec4-9a3a-8d4f794ca42b",
   "metadata": {},
   "source": [
    "Before starting with the model experimentation, a function was created that gathers the right data based on the input preferences for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c094c6f-053a-4d6c-94e4-013d17e1a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_features(df, model_params, classifier):\n",
    "    input_cols = []\n",
    "    for key, value in model_params.items():\n",
    "        if value == 1:\n",
    "            if key == \"tf_idf\":\n",
    "                input_cols.append(\"tfidf_features\")\n",
    "            elif key == \"comvot\":\n",
    "                if classifier == \"LogReg\":\n",
    "                    input_cols.append(\"norm_comvot\")\n",
    "                else:\n",
    "                    input_cols.append(\"comments\")\n",
    "                    input_cols.append(\"votes\")\n",
    "            elif key == \"Topic_modelling\":\n",
    "                input_cols.append(\"topicDistribution\")\n",
    "            elif key == \"word2vec\":\n",
    "                input_cols.append(\"w2vfeatures_rescaled\")\n",
    "\n",
    "    assembler = VectorAssembler(inputCols= input_cols, outputCol=\"features\")\n",
    "    df_final = assembler.transform(df)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e128a7c-2e7c-495c-ba18-0a57dce535df",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51863faa-efc5-4cc1-a55b-4d892b7a0d5f",
   "metadata": {},
   "source": [
    "#### NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b76b7840-4220-4685-ba51-26b69e731b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayesOwn(train):\n",
    "    nb = NaiveBayes(modelType=\"multinomial\", labelCol=\"frontpage\")\n",
    "    model = nb.fit(train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe08a10-c4be-49b2-90d5-f96b3935f2d9",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b2219af-1cad-4e61-8eeb-9328fab0ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(train):\n",
    "    # Create a LogisticRegression instance. This instance is an Estimator.\n",
    "    lr = LogisticRegression(maxIter=10, featuresCol = \"features\", labelCol = \"frontpage\")\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model = lr.fit(train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2bedf-4208-4ca6-8206-ee15dd176851",
   "metadata": {},
   "source": [
    "### PART 1: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4b4d771-e37d-4c99-bec4-28931d914aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc0774-8be9-44fb-a996-6cbd3026bb4b",
   "metadata": {},
   "source": [
    "To facilitate the comparison of various models, we designed a framework that enabled us to customize the preprocessing techniques and model parameters. The methodology involves creating the model with the desired inputs the desired, computation of metrics, and addition of results to the table. With this approach we can systematically evaluate the impact of different preprocessing methods and parameters on model performance, helping us choose the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69859fda-94cf-45be-ada1-2e171736aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\"tf_idf\" : 1, \"comvot\": 1, \"oversampling\" : 1}\n",
    "custom_params = {\"tf_idf\" : 10,\"comvot\" : \"\", \"oversampling\": \"\"}\n",
    "classifier = \"NaiveBayes\"\n",
    "train_processed = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133f19b-f533-479e-ba11-3a4378a653d4",
   "metadata": {},
   "source": [
    "#### Processing Pipeline and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e11eee55-a47f-46d9-b465-61a117350a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Majority:Minority ratio is 4.88:1\n",
      "Oversampling of minority done such that Majority:Minority ratio is 1:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if model_params[\"tf_idf\"] == 1:\n",
    "    train_processed = tf_idf(train_processed, \"words\")\n",
    "if classifier == \"LogReg\":\n",
    "    train_processed = normalization(train_processed, [\"comments\", \"votes\"])\n",
    "\n",
    "train_processed = gather_features(train_processed, model_params, classifier)\n",
    "train, test = traintestsplit(train_processed)\n",
    "if model_params[\"oversampling\"] == 1:\n",
    "    train = oversample_minority(train)\n",
    "\n",
    "if classifier == \"NaiveBayes\":\n",
    "    model = NaiveBayesOwn(train)\n",
    "elif classifier == \"LogReg\":\n",
    "    model = LogReg(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd903ed-8b51-484d-be9c-e3473ae7f713",
   "metadata": {},
   "source": [
    "#### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5bc69f2-efb3-4d5d-8868-685f9cd73546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 116:==================>                                    (7 + 14) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------------------------------+-------------------------------------------+----------+\n",
      "|features                                                                                                                                                                                                      |frontpage|rawPrediction                            |probability                                |prediction|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------------------------------+-------------------------------------------+----------+\n",
      "|[0.17868615809867577,0.2924250375691107,0.32316902023672717,0.1445542967248334,0.49020409609344395,0.2802736940068496,0.0,0.4773650184285337,0.22231952618759565,0.33236601346411593,0.0,1.0]                 |0        |[-11.540581973446509,-10.168989123030386]|[0.20236261938137984,0.7976373806186201]   |1.0       |\n",
      "|[1.7153871177472872,3.2654129195217365,9.002565563737399,1.3973582016733892,2.5599547240435405,2.662600093065071,2.0189929042336017,2.864190110571202,2.9457337219856425,1.3294640538564637,0.0,1.0]          |0        |[-70.72988846915528,-72.59321876233135]  |[0.8656846488112708,0.13431535118872923]   |0.0       |\n",
      "|[5.146161353241862,5.897238257643733,9.818182614811045,9.636953114988891,6.6994559799437345,7.0535546325057155,8.964328494797192,7.637840294856539,7.0586449564561615,6.148771249086145,0.0,1.0]              |0        |[-176.97661610105686,-185.69795403577487]|[0.9998369577177181,1.6304228228196903E-4] |0.0       |\n",
      "|[12.400819372048097,20.37227761731471,74.5443206679384,48.04021127821962,23.63873085606163,69.36773926669528,58.30851507426641,14.440291807463144,27.345301721074264,27.97413946656309,0.0,3.0]               |1        |[-842.8654150920385,-877.1164578099437]  |[0.9999999999999987,1.3334021411803305E-15]|0.0       |\n",
      "|[12.079184287470483,12.184376565379614,38.857227433225525,17.97291755945428,13.889116055980912,16.11573740539385,24.954752296327314,13.485561770606077,20.231076883071204,13.017668860677874,0.0,1.0]         |0        |[-415.43434384210155,-436.46155497803665]|[0.9999999992620987,7.379012287456359E-10] |0.0       |\n",
      "|[0.4288467794368218,0.6335875813997399,0.7848390491463374,0.5782171868993335,0.5991383396697648,0.46712282334474936,0.8075971616934406,0.5370356457321004,1.0004378678441803,0.5539433557735265,0.0,1.0]      |0        |[-19.837656665361482,-18.85144882476371] |[0.2716617505072454,0.7283382494927546]    |1.0       |\n",
      "|[5.324847511340538,5.702288232597659,9.17184457433759,7.275899601816613,5.0109752045107605,4.204105410102744,8.15673133310375,7.041134021820872,5.280088746955396,4.542335517342917,0.0,2.0]                  |0        |[-152.2811809005458,-157.88802427799592] |[0.9963407947281675,0.0036592052718324103] |0.0       |\n",
      "|[0.2858978529578812,0.34116254383062916,0.6155600385461469,0.4336628901745001,0.21786848715264176,0.18684912933789974,0.24227914850803217,0.417694391124967,0.3890591708282924,0.22157734230941062,0.0,2.0]   |0        |[-16.832496347540815,-13.645179551310829]|[0.03964581381731412,0.9603541861826859]   |1.0       |\n",
      "|[3.6094603935932503,4.72753810736729,10.249074641793348,4.0957050738702785,4.193968377688354,3.1764351987442954,8.479770197781127,7.995864058677939,5.057769220767801,4.930095866384386,0.0,1.0]              |0        |[-134.13004463164492,-140.76160287507287]|[0.9986836275679499,0.0013163724320500406] |0.0       |\n",
      "|[2.6802923714801365,2.9729878819526254,5.770875361370128,3.8547812459955564,5.283310813451562,3.40999661041667,8.31825076544244,5.549368339231704,4.446390523751913,4.763912859652328,0.0,2.0]                |0        |[-118.11465611018698,-121.45541206806226]|[0.9658008200849539,0.0341991799150461]    |0.0       |\n",
      "|[0.5360584742960273,0.536112568876703,1.1233970703467182,0.48184765574944455,0.5991383396697648,0.747396517351599,1.6151943233868813,0.5967062730356671,0.8892781047503826,0.3877603490414686,0.0,1.0]        |0        |[-21.892998068956253,-21.001105085874887]|[0.2907193367041558,0.7092806632958442]    |1.0       |\n",
      "|[0.03573723161973515,0.1949500250460738,0.40011402505499555,0.38547812459955566,0.16340136536448133,0.18684912933789974,0.4037985808467203,0.17901188191070014,0.3890591708282924,0.22157734230941062,0.0,6.0]|1        |[-31.57077717483814,-20.872618992662403] |[2.2585989706258636E-5,0.9999774140102938] |1.0       |\n",
      "|[9.684789768948226,14.426301853409463,34.47136215858423,19.12935193325295,12.30956952412426,15.835463711387003,25.03551201249666,11.03906605115984,13.116852045068143,9.140065370263189,0.0,1.0]              |0        |[-371.73833071886355,-390.70195409875856]|[0.9999999941896405,5.810359536115649E-9]  |0.0       |\n",
      "|[0.0,0.34116254383062916,0.4770590298732639,0.1445542967248334,0.3812698525171231,0.1401368470034248,0.0,0.11934125460713342,0.0,0.22157734230941062,0.0,11.0]                                                |1        |[-50.894433579967554,-31.028161853775188]|[2.3560679541659312E-9,0.999999997643932]  |1.0       |\n",
      "|[2.8232412979590773,2.7293003506450333,8.12539250880914,4.770291791919501,2.723356089408022,3.736982586757995,7.914452184595718,3.4608963836068694,3.334792892813935,3.046688456754396,0.0,1.0]               |0        |[-100.42828834412548,-104.21175906404639]|[0.9777621515637913,0.022237848436208654]  |0.0       |\n",
      "|[0.7147446323947031,0.9747501252303691,3.0931891936943887,1.686466795123056,0.49020409609344395,1.1678070583618734,1.292155458709505,0.4773650184285337,0.8892781047503826,0.3877603490414686,0.0,1.0]        |0        |[-29.254271472142086,-28.66667694585259] |[0.35718697181057696,0.6428130281894231]   |1.0       |\n",
      "|[8.005139882820675,9.650026239780654,16.66628804363693,9.974246474013503,8.878140851470151,11.397796889611884,16.878780679392907,7.936193431374372,9.337420099879017,10.192557746232888,0.0,2.0]              |0        |[-259.19780707397564,-270.4029430436522] |[0.9999863960392191,1.3603960780941188E-5] |0.0       |\n",
      "|[5.68221982753789,5.068700651197919,12.095754757431788,6.649497649342335,6.372653249214771,5.6054738801369925,10.256483953506695,7.160475276428005,8.281402350487937,7.09047495390114,0.0,1.0]                |0        |[-175.90593681507187,-184.44241399500123]|[0.9998038583274758,1.9614167252427736E-4] |0.0       |\n",
      "|[0.464584011056557,0.2924250375691107,1.5696780982926748,0.24092382787472227,0.3812698525171231,0.32698597634132454,0.9691165940321287,0.4773650184285337,0.16673964464069674,0.22157734230941062,0.0,1.0]    |0        |[-15.941101507887744,-14.79783740306309] |[0.24172157159110452,0.7582784284088955]   |1.0       |\n",
      "|[1.2865403383104654,2.144450275506812,3.3240242081491935,2.5537925754720563,1.7974150190092946,2.8027369400684963,4.522544105483267,3.281884501696169,2.2231952618759565,1.772618738475285,0.0,1.0]           |0        |[-63.77888175772948,-65.53748169940775]  |[0.8530342260579098,0.14696577394209023]   |0.0       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----------------------------------------+-------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 13:38:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "predictions = model.transform(test)\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6141209b-a59e-4f19-9aa6-fac449674203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:=======================================>               (15 + 6) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"frontpage\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test set accuracy = {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bef0db-9430-4caa-a4df-9b8cdf02e61b",
   "metadata": {},
   "source": [
    "The final analysis for deciding which of models will be used, will mostly be based on the tpr, since this is the most crucial aspect for the project. It is not important predicting that an article will not end up on the frontpage well, the crucial part is making sure the frontpages are predicted well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b25abee2-8585-4802-af72-9fe34e464736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "true_positive = predictions.filter((col(\"prediction\") == 1) & (col(\"frontpage\") == 1)).count()\n",
    "false_negative = predictions.filter((col(\"prediction\") == 0) & (col(\"frontpage\") == 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f6c58c2-601c-4250-b939-5312e5661548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR): 0.6506849315068494\n"
     ]
    }
   ],
   "source": [
    "if (true_positive + false_negative) > 0:\n",
    "    tpr = true_positive / (true_positive + false_negative)\n",
    "else:\n",
    "    tpr = 0.0  # Handle the case when there are no positive samples in the data\n",
    "\n",
    "print(f\"True Positive Rate (TPR): {tpr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8cf49-08a5-4217-9042-f8c5dd21da7f",
   "metadata": {},
   "source": [
    "#### Table of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92803f25-30fd-449a-b599-c3493b6211e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the row to add\n",
    "model_name = classifier\n",
    "for key, value in model_params.items():\n",
    "    if value == 1:\n",
    "        model_name = model_name + \"+\" + key + str(custom_params[key])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"Accuracy\", DoubleType(), True),\n",
    "    StructField(\"TPR\", DoubleType(), True)\n",
    "])\n",
    "# Create a DataFrame with the new row\n",
    "new_row = spark.createDataFrame([Row(Model=model_name, Accuracy=accuracy, TPR=tpr)], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf0edf99-9277-4356-acb6-d7e5009d6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_time == True:\n",
    "    # Create an empty DataFrame\n",
    "    empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "    updated_df = empty_df.union(new_row)\n",
    "    first_time = False\n",
    "else:\n",
    "    updated_df = updated_df.union(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac3a3944-33bf-4f73-9431-4724030ad58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------------+------------------+\n",
      "|Model                                  |Accuracy         |TPR               |\n",
      "+---------------------------------------+-----------------+------------------+\n",
      "|NaiveBayes+tf_idf10+comvot+oversampling|0.710960960960961|0.6506849315068494|\n",
      "+---------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d0cef-38ba-49d4-9a5d-6d3fb871a7e9",
   "metadata": {},
   "source": [
    "The first model that was developped was the NaiveBayes with variables comments and votes, which served as the baseline model. This model returned an accuracy of 0.75, and a tpr of 0.78, which was more than expected. Later, the idea for expanding the model was including oversampling. After running the baseline model with oversampling, this improved both acuracy and tpr, increasing to 0.8 and 0.86 each. This suggests oversampling really helped in predicting positives well. The next step was adding TF-IDF counts as input features. After running models with increasing tf-idf features, starting from 10 and ending in 1000, it can be observed that the more the tf-idf features, the lower the tpr. It starts with a tpr of 0.65 for 10 tf-idf features, which is already a huge decrease, and it ends up at a worrying tpr of 0.3.The decrease in true positive rate (TPR) as more TF-IDF features are input into a Naive Bayes model can be traced back to the independence assumption in Naive Bayes classifiers. Naive Bayes assumes that all features (words, in this case) are independent given the class label. This means it treats the presence or absence of each word in isolation.\n",
    "\n",
    "However, this is a strong assumption and often not true in real-world text data, where the combination of words (context) is important. In reality, the relationship between a specific word and whether an article will end up on the front page or not is likely to have very little correlation. It is unlikely that a unique word on its own can determine this outcoeg.\n",
    "\n",
    "When the Naive Bayes model multiplies all these small probabilities for each word occurring independently, it often results in extremely small values. This process and bias the classifier towards predicting the majority class (often 0), thereby reducing the number of true poses.\n",
    "\n",
    "For Logistic regression the same building model process was used, with the slight difference that normalization had to be applied, because the majority of the model's predictions were 0. The baseline Logistic Regression showed worse results than the baseline Naive Bayes, with an accuracy of 0.58 and a tpr of 0.62. The Logistic regression model with oversampling improves a lot and is almost as good as the Naive Bayes with oversampling. For the tf-idf effect on Logistic Regression, we get almost the same results as for the Naive Bayes. The best model after running Logistic Regressions and Naive Bayes' with oversampling and tf-idf, is the baseline Naive Bayes model with oversampling. This means that the best model does not make use of textual data, which is a shame, since this data could potentially be very useful. itiv\n",
    "As a result, more dense representations (lower dimensionality) of the context were needed, and compared to TF-IDF, word2vec considers the relationship between words. It is also cost-efficient in calculation, unlike TF-IDF, which ca our computers touses craTopic-modelling using Latent Dirichtlet Allocation was also added, since it was hypothesized that certain topics could definitely have a higher popularity and result in a higher ratio of frontpage articles (e.g ai currently, bitcoin 5 years ago, etc). ffect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad848c-7890-40c7-9fec-4929ff6a74b1",
   "metadata": {},
   "source": [
    "### PART 2: Word2Vec and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09935536-5d32-4575-9e6a-4e2eccb14c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9548aa6-2446-4f56-a377-29187c9016b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\"Topic_modelling\": 0, \"word2vec\" : 1, \"comvot\" : 1, \"oversampling\" : 1}\n",
    "custom_params = {\"Topic_modelling\" : 3, \"word2vec\" : 5, \"comvot\" : \"\", \"oversampling\": \"\"}\n",
    "classifier = \"LogReg\"\n",
    "train_processed = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad92eeb-496e-40e0-88e8-612af4f292d8",
   "metadata": {},
   "source": [
    "#### Processing Pipeline and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8e44e92-20c5-4fd5-bb6b-cdf44008584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Majority:Minority ratio is 4.88:1\n",
      "Oversampling of minority done such that Majority:Minority ratio is 1:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 311:=======================================>               (15 + 6) / 21]\r"
     ]
    }
   ],
   "source": [
    "if model_params[\"Topic_modelling\"] == 1:\n",
    "    train_processed = lda_met(train_processed, \"words\", custom_params[\"Topic_modelling\"])\n",
    "if model_params[\"word2vec\"] == 1:\n",
    "    train_processed = word2vecproc(train_processed, \"words\", custom_params[\"word2vec\"])\n",
    "if classifier == \"LogReg\":\n",
    "    train_processed = normalization(train_processed, [\"comments\", \"votes\"])\n",
    "\n",
    "train_processed = gather_features(train_processed, model_params, classifier)\n",
    "train, test = traintestsplit(train_processed)\n",
    "if model_params[\"oversampling\"] == 1:\n",
    "    train = oversample_minority(train)\n",
    "\n",
    "if classifier == \"NaiveBayes\":\n",
    "    model = NaiveBayesOwn(train)\n",
    "elif classifier == \"LogReg\":\n",
    "    model = LogReg(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9dc6b3-6e32-487f-97c5-8e00ff599523",
   "metadata": {},
   "source": [
    "#### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "befc5f37-0c45-4fa7-8873-9b845ab8b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 326:=======================================>               (15 + 6) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------+------------------------------------------+-----------------------------------------+----------+\n",
      "|features                                                                                                                               |frontpage|rawPrediction                             |probability                              |prediction|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------+------------------------------------------+-----------------------------------------+----------+\n",
      "|[0.7710486538347004,0.9462704667386957,0.225409274098229,0.9933298658271144,0.07910426463851522,0.0,1.0]                               |1        |[0.13520491134802626,-0.13520491134802626]|[0.5337498302179107,0.46625016978208933] |0.0       |\n",
      "|[0.5772420746471496,0.42959982333068086,0.5326980887757524,0.7630544676790549,0.5158377467943263,0.11914522061843064,0.992876838486922]|1        |[-1.0886331836927035,1.0886331836927035]  |[0.25187574618397024,0.7481242538160298] |1.0       |\n",
      "|[0.43494031256025434,0.5406496568709398,0.536781210752156,0.7430456777279538,0.4132358176533016,0.0,1.0]                               |0        |[0.4392183092347146,-0.4392183092347146]  |[0.608072753730605,0.391927246269395]    |0.0       |\n",
      "|[0.41685457526361475,0.8451150975255817,0.4213832752471158,0.9168295811644906,0.2464541161166349,0.0,1.0]                              |0        |[0.1287886447447235,-0.1287886447447235]  |[0.5321527316530494,0.46784726834695056] |0.0       |\n",
      "|[0.44794451801691937,0.5911209901584973,0.49366032710276797,0.7337096813031829,0.5543987956509486,0.0,1.0]                             |0        |[0.4915832706245915,-0.4915832706245915]  |[0.6204793394513383,0.37952066054866174] |0.0       |\n",
      "|[0.6439206360886295,0.5001012786249192,0.525217415687787,0.6964075894039103,0.36092509444929305,0.0,1.0]                               |0        |[0.5358679430140612,-0.5358679430140612]  |[0.630850673517604,0.36914932648239596]  |0.0       |\n",
      "|[0.6192116149442346,0.5146372067466972,0.539935701339318,0.7038747651108507,0.384636436400765,0.0,1.0]                                 |0        |[0.5393707941633359,-0.5393707941633359]  |[0.63166603628659,0.36833396371341]      |0.0       |\n",
      "|[0.5521902890236763,0.7395327533209567,0.592723747520372,0.8301038852644544,0.24921789489305413,0.40613846605344767,0.9138115486202573]|1        |[-4.071533133717285,4.071533133717285]    |[0.016765356749540963,0.9832346432504591]|1.0       |\n",
      "|[0.5847588520972148,0.5394343542207514,0.554446992864966,0.6910521196162073,0.2699202480247547,0.0,1.0]                                |0        |[0.393136185987629,-0.393136185987629]    |[0.5970374437046783,0.40296255629532174] |0.0       |\n",
      "|[0.44684317151685504,0.5975135338775529,0.49264946653155794,0.7340965800335231,0.5467992705272141,0.0,1.0]                             |0        |[0.47883086126101304,-0.47883086126101304]|[0.6174717617017823,0.3825282382982177]  |0.0       |\n",
      "|[0.5635335265593514,0.4560409307511125,0.5311233236468386,0.7599972055442485,0.46240257339542146,0.0,1.0]                              |0        |[0.6604559748348535,-0.6604559748348535]  |[0.6593628094336019,0.3406371905663981]  |0.0       |\n",
      "|[0.5987841743064339,0.39892512687778287,0.5536792964228986,0.7737633347511215,0.4793596017785588,0.0,1.0]                              |0        |[0.7740259801299132,-0.7740259801299132]  |[0.6843911499208606,0.3156088500791394]  |0.0       |\n",
      "|[0.6417119665747407,0.519481859120562,0.6312227996034588,0.6845906148848748,0.16963329600328367,0.7682212795973759,0.6401843996644799] |1        |[-2.0856412211001434,2.0856412211001434]  |[0.11050027177616335,0.8894997282238366] |1.0       |\n",
      "|[0.641039639826228,0.557498342558308,0.6305593674662501,0.702425722989237,0.2530719577603693,0.0,1.0]                                  |0        |[0.45543633493188906,-0.45543633493188906]|[0.6119309911513032,0.3880690088486968]  |0.0       |\n",
      "|[0.6161846367395175,0.5859250447232028,0.67102446565499,0.7069208703061575,0.18606117542749134,0.0,1.0]                                |0        |[0.3823259583960379,-0.3823259583960379]  |[0.5944339731008607,0.40556602689913934] |0.0       |\n",
      "|[0.6979747309233758,0.4486094413616901,0.5824937499126408,0.7717524077973524,0.41498059159009665,0.1414213562373095,0.9899494936611665]|1        |[-1.3600034716338385,1.3600034716338385]  |[0.20423973805321136,0.7957602619467886] |1.0       |\n",
      "|[0.6258407905749472,0.580994295696009,0.6367247251109848,0.7046972122044807,0.14090817420402893,0.0,1.0]                               |0        |[0.33640525586211467,-0.33640525586211467]|[0.5833170531968539,0.4166829468031461]  |0.0       |\n",
      "|[0.6083748630695733,0.5746510720319972,0.579240442373621,0.6836130946284296,0.3054196252548571,0.0,1.0]                                |1        |[0.41582796082867546,-0.41582796082867546]|[0.6024844877810369,0.3975155122189631]  |0.0       |\n",
      "|[0.4528081180821181,0.6227920649233751,0.657352803885419,0.8345563533431486,0.30382943043509025,0.0,1.0]                               |0        |[0.43914983104588146,-0.43914983104588146]|[0.6080564338687043,0.39194356613129566] |0.0       |\n",
      "|[0.622655155555878,0.4126213135596028,0.5888418061432125,0.7377665780808161,0.4512653071885648,0.0,1.0]                                |1        |[0.7414159555605444,-0.7414159555605444]  |[0.6773054091026102,0.32269459089738983] |0.0       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+---------+------------------------------------------+-----------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make Predictions\n",
    "predictions = model.transform(test)\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "637959b1-972d-4bda-a112-71b5bc2cc376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 345:====================================================>  (20 + 1) / 21]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"frontpage\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test set accuracy = {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5d91e-da46-4b2f-881f-dc4e1e78e508",
   "metadata": {},
   "source": [
    "The final analysis for deciding which of models will be used, will mostly be based on the tpr, since this is the most crucial aspect for the project. It is not important predicting that an article will not end up on the frontpage well, the crucial part is making sure the frontpages are predicted well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14cf2e29-c20b-4e08-b7be-f1f17a73be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "true_positive = predictions.filter((col(\"prediction\") == 1) & (col(\"frontpage\") == 1)).count()\n",
    "false_negative = predictions.filter((col(\"prediction\") == 0) & (col(\"frontpage\") == 1)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8961a19d-287f-4a2c-843c-1b13aa380c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR): 0.26811594202898553\n"
     ]
    }
   ],
   "source": [
    "if (true_positive + false_negative) > 0:\n",
    "    tpr = true_positive / (true_positive + false_negative)\n",
    "else:\n",
    "    tpr = 0.0  # Handle the case when there are no positive samples in the data\n",
    "\n",
    "print(f\"True Positive Rate (TPR): {tpr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf160c13-441a-4c2d-a308-05b6c4384469",
   "metadata": {},
   "source": [
    "#### Table of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2644302-14cd-4ad8-9d41-cf87fdd81dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the row to add\n",
    "model_name = classifier\n",
    "for key, value in model_params.items():\n",
    "    if value == 1:\n",
    "        model_name = model_name + \"+\" + key + str(custom_params[key])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Model\", StringType(), True),\n",
    "    StructField(\"Accuracy\", DoubleType(), True),\n",
    "    StructField(\"TPR\", DoubleType(), True)\n",
    "])\n",
    "# Create a DataFrame with the new row\n",
    "new_row = spark.createDataFrame([Row(Model=model_name, Accuracy=accuracy, TPR=tpr)], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b50ac611-b9a0-4bf4-83b2-bc28a99121fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_time == True:\n",
    "    # Create an empty DataFrame\n",
    "    empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "    updated_df2 = empty_df.union(new_row)\n",
    "    first_time = False\n",
    "else:\n",
    "    updated_df2 = updated_df2.union(new_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9797f66-13ee-48ab-99fc-cae3d9483783",
   "metadata": {},
   "source": [
    "The models tried in this second part seem to improve accuracy, but the TPR is far from desired. For Naive Bayes, Topic Modelling shows a very low tpr of 0.35, while word2vec embeddings show an impressive 87 percent accuracy but a low tpr of 0.46. Different sizes of word2vec embeddings were tried, and higher dimensional ones seem to yield a better result. For logistic regression, the results were worse than the NaiveBayes, both in tpr and accuracy, for both techniques. Both techniques (Word2Vec and Topic Modelling) couldn't be combined together, this caused the pc to crash. In summary, it was difficult to further develop and improve the models that we built. This was mainly due to the fact that our laptops could not handle the training of these models. Something that we wanted to do, but could not due to limited computational power is perform hyperparameter tuning of the number of word2vec embeddings and the number of topics in LDA. \n",
    "\n",
    "Other future work involved including title and domain data. The title could have been a useful variable. People mainly click articles because of attractive titles. It was hard to devise a strategy to quantify this attractiveness. Bert models could have been very useful in our case, to extract these types of patterns from embeddings. Different variants of models like Xgboost and RandomForest could also have been used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef81d63-269d-4686-a2c7-ea7813c2fe98",
   "metadata": {},
   "source": [
    "### Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b4736-3a0e-4840-9575-2629d86bd712",
   "metadata": {},
   "source": [
    "For deployment, the chosen model was the baseline NaiveBayes with oversampling, since it returned the highest tpr (0.86)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c822cc8e-0ab3-4ceb-82ac-7f048bb358ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Majority:Minority ratio is 4.88:1\n",
      "Oversampling of minority done such that Majority:Minority ratio is 1:1\n"
     ]
    }
   ],
   "source": [
    "model_params = {\"tf_idf\" : 1, \"comvot\": 1, \"oversampling\" : 1}\n",
    "custom_params = {\"tf_idf\" : 10,\"comvot\" : \"\", \"oversampling\": \"\"}\n",
    "classifier = \"NaiveBayes\"\n",
    "\n",
    "if model_params[\"tf_idf\"] == 1:\n",
    "    train_processed = tf_idf(train_processed, \"words\")\n",
    "train_processed = gather_features(train_processed, model_params, classifier)\n",
    "train, test = traintestsplit(train_processed)\n",
    "train = oversample_minority(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d82621d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol='features', modelType=\"multinomial\", labelCol=\"frontpage\")\n",
    "nbm = nb.fit(train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67ed6d54-fc57-40d9-8662-dcb2e5c39fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to the local file, please define own local directory here\n",
    "\n",
    "model_path = './models/naive_bayes_finalmodel'\n",
    "naive_bayes_final = nbm.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8adbdf4-60fe-4c58-8f90-7a3836c860dc",
   "metadata": {},
   "source": [
    "## Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70ca762-0172-4033-b1ea-998c1e49dec7",
   "metadata": {},
   "source": [
    "There are two goals:\n",
    "(1) save the model;\n",
    "(2) preprocessing the incoming message.\n",
    "In the model deployment the preprocessing and tokenization steps are basically repeated to featurize the incoming stream data so that the model could understand the data. However, for all the tools we did not refit them in combination of the new data and the original data set. The new data is decomposited using the tools fitted on the original data set only. For example, a new entry would be tokenized and transformed based on the transformer fitted using the train set. A new word from the entry would not be taken into account since the transformer does not have knowledge on that new word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bab3fa3-ef7c-457a-9779-e8f8a37e7d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97ef99a8-bd92-4d5d-996d-fc224c1152ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a9e1409-98a9-4c50-800e-23861261dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6cd5f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|comments|frontpage|         source_text|votes|              tokens|               words|         tf_features|      tfidf_features|\n",
      "+--------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|       0|        0| HORS S RIE  6  T...|    1|[, hors, s, rie, ...|[, hors, rie, , 6...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
      "|       0|        0|Ubuntu 24 04  Nob...|    1|[ubuntu, 24, 04, ...|[ubuntu, 24, 04, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
      "|       0|        0|Weird monitor bug...|    1|[weird, monitor, ...|[weird, monitor, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
      "|       0|        0|SQL Optimizations...|    2|[sql, optimizatio...|[sql, optimizatio...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
      "|       0|        0|The Worst Product...|    1|[the, worst, prod...|[worst, product, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|\n",
      "+--------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_stream = HashingTF(inputCol= 'words', outputCol=\"tf_features\", numFeatures=10)\n",
    "tf_data = hashing_stream.transform(df_clean)\n",
    "\n",
    "idf_stream = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "idf_model_stream = idf_stream.fit(tf_data)\n",
    "df_tfidf = idf_model_stream.transform(tf_data)\n",
    "df_tfidf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "665e01aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|comments|frontpage|         source_text|votes|              tokens|               words|         tf_features|      tfidf_features|            features|\n",
      "+--------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       0|        0| HORS S RIE  6  T...|    1|[, hors, s, rie, ...|[, hors, rie, , 6...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[7.54055587176411...|\n",
      "|       0|        0|Ubuntu 24 04  Nob...|    1|[ubuntu, 24, 04, ...|[ubuntu, 24, 04, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[16.7607616296557...|\n",
      "|       0|        0|Weird monitor bug...|    1|[weird, monitor, ...|[weird, monitor, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[3.53798593035378...|\n",
      "|       0|        0|SQL Optimizations...|    2|[sql, optimizatio...|[sql, optimizatio...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[9.00578236817325...|\n",
      "|       0|        0|The Worst Product...|    1|[the, worst, prod...|[worst, product, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.17997112880384...|\n",
      "|       0|        0|The security impl...|    2|[the, security, i...|[security, implic...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.93045299281828...|\n",
      "|       0|        0|GitHub   soywod n...|    1|[github, , , soyw...|[github, , , soyw...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.53670095964861...|\n",
      "|       0|        0|Filesystem error ...|    2|[filesystem, erro...|[filesystem, erro...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[11.9004981293718...|\n",
      "|       0|        0|OpenRazer 3 8 Bri...|    2|[openrazer, 3, 8,...|[openrazer, 3, 8,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.67964988612755...|\n",
      "|       0|        0|GitHub   sjurba r...|    2|[github, , , sjur...|[github, , , sjur...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[4.53862841570636...|\n",
      "|       0|        0|How Perfectly Can...|    2|[how, perfectly, ...|[perfectly, reali...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[25.9452301559277...|\n",
      "|       6|        1|Are Flying Cars F...|    8|[are, flying, car...|[flying, cars, fi...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[21.9783974461371...|\n",
      "|       0|        0|GCC version 14 co...|    1|[gcc, version, 14...|[gcc, version, 14...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[15.6886446810637...|\n",
      "|       0|        0|Malleable softwar...|    2|[malleable, softw...|[malleable, softw...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[12.0791842874704...|\n",
      "|       0|        0|Research finds Am...|    2|[research, finds,...|[research, finds,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[16.0817542288808...|\n",
      "|       0|        0|Cammy Bikeshed   ...|    1|[cammy, bikeshed,...|[cammy, bikeshed,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.67964988612755...|\n",
      "|       0|        0|RVBBIT    First R...|    1|[rvbbit, , , , fi...|[rvbbit, , , , fi...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[4.03830717303007...|\n",
      "|       0|        0|Argentina s Javie...|    1|[argentina, s, ja...|[argentina, javie...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[12.1149215190902...|\n",
      "|       0|        0|Verse  A New Func...|    1|[verse, , a, new,...|[verse, , new, fu...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[4.39567948922742...|\n",
      "|       0|        1|Python is Actuall...|   21|[python, is, actu...|[python, actually...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[4.64584011056557...|\n",
      "+--------+---------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"tfidf_features\", \"comments\", \"votes\"], outputCol=\"features\", handleInvalid=\"keep\")\n",
    "vecAssembler.transform(df_tfidf).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb5be764-0de4-46d3-9e4d-657912ad5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "# The final function\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df_stream = spark.read.json(rdd)\n",
    "    df_stream = df_stream\n",
    "    # df_stream.show()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    df_punc_drop_stream = df_stream.withColumn('source_text', regexp_replace(df_stream.source_text, '[^a-zA-Z0-9]', ' '))\n",
    "    #df_punc_drop_stream.show()\n",
    "    \n",
    "    # Transformed with tokens\n",
    "    df_token_stream = Tokenizer(inputCol=\"source_text\", outputCol=\"tokens\").transform(df_punc_drop_stream)\n",
    "    #df_token_stream.show()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df_clean_stream = stopwords.transform(df_token_stream)\n",
    "    #df_clean_stream.show()\n",
    "    \n",
    "    # Apply Word2Vec\n",
    "    # result_stream = word2vec_model.transform(df_clean_stream)\n",
    "\n",
    "    # Apply TF-IDF\n",
    "    tfData_stream = hashing_stream.transform(df_clean_stream)\n",
    "    idfData_stream = idf_model_stream.transform(tfData_stream)\n",
    "    \n",
    "    # Combine comments and votes with tfidf_features\n",
    "    assemble_df = vecAssembler.transform(idfData_stream)\n",
    "    \n",
    "    # Finalized the training data\n",
    "    #training_stream = assemble_df.drop('tokens','words')\n",
    "    #training_stream.show()\n",
    "\n",
    "    # Make predictions with the selected model\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = NaiveBayesModel.load(model_path)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model:\n",
    "    df_result = globals()['my_model'].transform(assemble_df)\n",
    "    df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6db464-0cf1-4607-9243-98c60a8c811d",
   "metadata": {},
   "source": [
    "## Streaming prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8861b4a3",
   "metadata": {},
   "source": [
    "After defining the function, now the socket is connected. With each batch of data comming in, there is a prediction accordingly. Both probabilities and predicted classification are shown at the end of each entry. As seen from the result, most entries without appearing on the frontpage are predicted correctly. For some entries with appearing on the frontpage, the model could give a correct prediction as well. Right below the code are some examples of streaming prediction. Once there is a new entry, a column 'prediciton' is added at the end of the entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "914382e2-8e5a-48c4-b7cc-bc7ce388cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f706a4c-a545-4513-acb0-46fe490113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49c4bfc6-7f04-46c4-8b63-9f26859584f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-26 14:45:30 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|    user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478937|       0|  textslashplain.com|    false|2024-05-26 00:48:00|Authenticode in 2...|Authenticode in 2024|Authenticode in 2024|https://textslash...|aa_is_op|    1|[authenticode, in...|[authenticode, 20...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[3.57372316197351...|[-125.29221758363...|[0.99818650610681...|       0.0|\n",
      "|40478948|       0|befinitiv.wordpre...|    false|2024-05-26 00:50:16|8cm Portable Blur...|8cm Portable Blur...|8cm Portable Blur...|https://befinitiv...|     zdw|    2|[8cm, portable, b...|[8cm, portable, b...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.42948926478940...|[-49.444883619020...|[0.49842517494717...|       1.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:45:40 =========\n",
      "+--------+--------+-------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|             domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|        user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40478951|       0|    theguardian.com|     true|2024-05-26 00:50:36| A catastrophe   ...|â€˜A catastropheâ€™: ...|'A catastrophe': ...|https://www.thegu...| pantalaimon|    4|[, a, catastrophe...|[, catastrophe, ,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.82324129795907...|[-102.43472181630...|[0.26457445600038...|       1.0|\n",
      "|40478963|       0|        latimes.com|    false|2024-05-26 00:52:21|Patt Morrison  Pa...|Patt Morrison: Pa...|Palos Verdes Peni...|https://www.latim...|     dangle1|    1|[patt, morrison, ...|[patt, morrison, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[6.57565061803126...|[-174.91142056599...|[0.99992782985728...|       0.0|\n",
      "|40478976|       5|       computer.rip|     true|2024-05-26 00:54:23|2024 05 25 grc sp...|2024-05-25 grc sp...|        GRC SpinRite|https://computer....|todsacerdoti|   11|[2024, 05, 25, gr...|[2024, 05, 25, gr...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[8.39824943063776...|[-399.20112278739...|[5.69328295724551...|       1.0|\n",
      "|40478979|       0|theconversation.com|    false|2024-05-26 00:55:14|Why you shouldn t...|Why you shouldnâ€™t...|You shouldn't tak...|https://theconver...|     gnabgib|    1|[why, you, should...|[shouldn, take, p...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.14423389718410...|[-73.873461826854...|[0.97276165347991...|       0.0|\n",
      "|40478992|       0|    breckyunits.com|    false|2024-05-26 00:57:28|What can we learn...|What can we learn...|What can we learn...|https://breckyuni...|       breck|    1|[what, can, we, l...|[learn, programmi...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.21506587507099...|[-39.202710296183...|[0.82582114056573...|       0.0|\n",
      "|40479023|       0|            edsa.uk|    false|2024-05-26 01:07:01|The KCS Power PC ...|The KCS Power PC ...|The KCS Power PC ...|https://www.edsa....|   sys_64738|    2|[the, kcs, power,...|[kcs, power, pc, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[6.61138784965100...|[-230.89824873376...|[0.99995195123818...|       0.0|\n",
      "+--------+--------+-------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:45:50 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jorge\\anaconda3\\envs\\assignment3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_33296\\265281298.py\", line 11, in run\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\streaming\\context.py\", line 239, in awaitTermination\n",
      "    self._jssc.awaitTermination()\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o391.awaitTermination.\n",
      ": org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\streaming\\util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"C:\\Users\\jorge\\AppData\\Local\\Temp\\ipykernel_33296\\2585906579.py\", line 50, in process\n",
      "    df_result.show()\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py\", line 945, in show\n",
      "    print(self._show_string(n, truncate, vertical))\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py\", line 963, in _show_string\n",
      "    return self._jdf.showString(n, 20, vertical)\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\jorge\\OneDrive\\Escritorio\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o855.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 139.0 failed 1 times, most recent failure: Lost task 1.0 in stage 139.0 (TID 11096) (192.168.1.14 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`Tokenizer$$Lambda$6247/0x00000001014cd840`: (string) => array<string>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n",
      "\t... 20 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat com.sun.proxy.$Proxy39.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`Tokenizer$$Lambda$6247/0x00000001014cd840`: (string) => array<string>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\t... 3 more\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n",
      "\t... 20 more\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2024-05-26 14:46:00 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|           user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479045|       0|         youtube.com|    false|2024-05-26 01:12:41|Pentium Pro  was ...|Pentium Pro, was ...|Pentium Pro, was ...|https://www.youtu...|     st_goliath|    1|[pentium, pro, , ...|[pentium, pro, , ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.71602960309987...|[-116.16537496700...|[0.99694501824414...|       0.0|\n",
      "|40479048|       0|github.com/crazyl...|    false|2024-05-26 01:12:53|GitHub   Crazyleg...|GitHub - Crazyleg...|JQuickTake: Java-...|https://github.co...|classichasclass|    2|[github, , , craz...|[github, , , craz...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[7.07597186070756...|[-224.00130705079...|[0.99997402915715...|       0.0|\n",
      "|40479049|       0|hollywoodreporter...|     true|2024-05-26 01:13:15|Hollywood Loan Ou...|Are Hollywood Loa...|California to end...|https://www.holly...|         Andrex|    5|[hollywood, loan,...|[hollywood, loan,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[8.82709621007458...|[-260.71547375524...|[0.99540755384052...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:46:10 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|      user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479051|       0|          nature.com|     true|2024-05-26 01:13:51|Who will make Alp...|Who will make Alp...|Who will make Alp...|https://www.natur...|panagathon|    3|[who, will, make,...|[make, alphafold3...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[7.00449739746809...|[-190.19428689021...|[0.99814066176980...|       0.0|\n",
      "|40479062|       0|medium.com/edmoor...|    false|2024-05-26 01:15:50|Just a moment    ...|    Just a moment...|Things I've learn...|https://medium.co...| andsoitis|    2|[just, a, moment,...|[moment, , , , , ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[0.14294892647894...|[-11.921026559879...|[0.11706728480936...|       1.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:46:20 =========\n",
      "+--------+--------+--------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|        domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url| user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479071|       0|janestreet.com|    false|2024-05-26 01:17:24|Jane Street Tech ...|Why GADTs matter ...|Why GADTs matter ...|https://blog.jane...|nequo|    1|[jane, street, te...|[jane, street, te...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[4.35994225760768...|[-179.09592711095...|[0.99996164019241...|       0.0|\n",
      "+--------+--------+--------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:46:50 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|      user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479097|       0|        futurism.com|    false|2024-05-26 01:20:42|Another OpenAI Re...|Another OpenAI Re...|Another OpenAI Re...|https://futurism....|     croes|    1|[another, openai,...|[another, openai,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.32292005528278...|[-65.468940990725...|[0.95908508441275...|       0.0|\n",
      "|40479107|       0|      chebucto.ns.ca|    false|2024-05-26 01:23:22|Doctor DOS Websit...|Doctor DOS Websit...|Doctor DOS Betama...|http://www.chebuc...|     Lammy|    1|[doctor, dos, web...|[doctor, dos, web...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[0.25016062133814...|[-17.366173710799...|[0.60284272077741...|       0.0|\n",
      "|40479113|       0|    flyingpigapp.com|    false|2024-05-26 01:24:42|Flying Pig   Hang...|Flying Pig - Hang...|How do you meet n...|https://flyingpig...|    antves|    1|[flying, pig, , ,...|[flying, pig, , ,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[0.67900740077496...|[-30.216504634799...|[0.75345228640477...|       0.0|\n",
      "|40479134|       0|twitter.com/intan...|    false|2024-05-26 01:28:53|X  Don t miss wha...|                   X|Trump says he wil...|https://twitter.c...|monero-xmr|    1|[x, , don, t, mis...|[x, , miss, happe...|(10,[0,1,2,3,6,9]...|(10,[0,1,2,3,6,9]...|[0.03573723161973...|[-5.5232419033167...|[0.42952600994076...|       1.0|\n",
      "|40479167|       0|bloodinthemachine...|     true|2024-05-26 01:36:29|Why is Sam Altman...|Why is Sam Altman...|Why is Sam Altman...|https://www.blood...|  grugagag|    5|[why, is, sam, al...|[sam, altman, obs...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[5.25337304810106...|[-184.47333486738...|[0.81312898790001...|       0.0|\n",
      "|40479169|       0|         nbcnews.com|    false|2024-05-26 01:36:53|Trump booed and j...|Trump booed and j...|Trump to commute ...|https://www.nbcne...| hienyimba|    2|[trump, booed, an...|[trump, booed, je...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[8.25530050415882...|[-221.69571997904...|[0.99991145671301...|       0.0|\n",
      "|40479171|       0|           arxiv.org|    false|2024-05-26 01:37:40| 2405 13926  Some...|Some models are u...|Some models are u...|https://arxiv.org...|     nequo|    1|[, 2405, 13926, ,...|[, 2405, 13926, ,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.96554773908543...|[-72.982504747203...|[0.97382232472146...|       0.0|\n",
      "|40479180|       0|github.com/microsoft|    false|2024-05-26 01:39:59|Ohh   WinUI3 is r...|Ohh...WinUI3 is r...|    Is WinUI 3 Dead?|https://github.co...| aragonite|    1|[ohh, , , winui3,...|[ohh, , , winui3,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[24.9445876705751...|[-766.06620114556...|[1.0,5.1284971426...|       0.0|\n",
      "|40479190|       0|    atlasobscura.com|    false|2024-05-26 01:42:01|The Quest for the...|The Quest for the...|The Quest for the...|https://www.atlas...|    Anon84|    1|[the, quest, for,...|[quest, real, lif...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[20.6203826445871...|[-1054.2228007115...|[1.0,1.5399945978...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:47:00 =========\n",
      "+--------+--------+------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|            domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|     user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479200|       0|nationalreview.com|    false|2024-05-26 01:43:59|Our Listless Univ...|Our Listless Univ...|Our Listless Univ...|https://www.natio...|blueridge|    2|[our, listless, u...|[listless, univer...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[13.8660458684572...|[-455.01061658297...|[0.99999999989202...|       0.0|\n",
      "+--------+--------+------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+---------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:47:10 =========\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ec9618f-9f78-438a-8ed9-cbf4a2d14d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479203|       0|        uptownhr.com|    false|2024-05-26 01:44:49|Must know key bin...|Must know key bin...|Must know key bin...|https://uptownhr....|  todsacerdoti|    1|[must, know, key,...|[must, know, key,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.03637971697231...|[-48.184789100787...|[0.89110455181503...|       0.0|\n",
      "|40479223|       0|          reddit.com|    false|2024-05-26 01:49:36|Blocked    whoa t...|             Blocked|News Suck So shou...|https://old.reddi...|  josevictorbp|    1|[blocked, , , , w...|[blocked, , , , w...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[0.25016062133814...|[-13.549774282814...|[0.54271934479098...|       0.0|\n",
      "|40479244|       0|   pertourmarket.com|    false|2024-05-26 01:54:29|Discover the Wild...|Discover the Wild...|Discover the Wild...|https://www.perto...|       Mmorgun|    1|[discover, the, w...|[discover, wildli...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[3.68093485683272...|[-78.552291419344...|[0.97879458219441...|       0.0|\n",
      "|40479253|       0|             npr.org|    false|2024-05-26 01:57:18|Paper books vs  e...|Whatâ€™s better for...|What's better for...|https://www.npr.o...|     rfarley04|    1|[paper, books, vs...|[paper, books, vs...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[8.68414728359564...|[-229.65766566442...|[0.99999493584175...|       0.0|\n",
      "|40479255|       0|         sturppy.com|    false|2024-05-26 01:57:28| 1 Financial Mode...|#1 Financial Mode...|             Sturppy|https://www.sturp...|handfuloflight|    1|[, 1, financial, ...|[, 1, financial, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.35865728690252...|[-88.557312391073...|[0.98871806952621...|       0.0|\n",
      "|40479261|       0|gripsintelligence...|    false|2024-05-26 01:58:54|Grips   Transacti...|Grips - Transacti...|               Grips|https://gripsinte...|handfuloflight|    1|[grips, , , trans...|[grips, , , trans...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[1.35801480154993...|[-49.173864785716...|[0.90659719407216...|       0.0|\n",
      "|40479284|       0|         youtube.com|    false|2024-05-26 02:05:47|Icing the Musical...|Icing the Musical...|Icing the Musical...|https://www.youtu...|  maroonblazer|    1|[icing, the, musi...|[icing, musical, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[4.71731457380504...|[-137.43394302046...|[0.99936259249412...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:47:20 =========\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|        user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479316|       0|         brickit.app|    false|2024-05-26 02:15:06|Brickit   Build n...|Brickit â€” Build n...|Brickit App Sugge...|https://brickit.app/|thepuppet33r|    1|[brickit, , , bui...|[brickit, , , bui...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[0.71474463239470...|[-26.528863753187...|[0.70222226844865...|       0.0|\n",
      "|40479328|       0|alchemy.substack.com|     true|2024-05-26 02:17:23|The Algorithm Beh...|The Algorithm Beh...|The Algorithm Beh...|https://alchemy.s...|       SirLJ|    4|[the, algorithm, ...|[algorithm, behin...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[4.68157734218530...|[-159.84492984345...|[0.91831102604320...|       0.0|\n",
      "|40479333|       0|         carbuzz.com|    false|2024-05-26 02:18:13|NHTSA Finds More ...|NHTSA Finds More ...|NHTSA Finds More ...|https://carbuzz.c...|    harambae|    1|[nhtsa, finds, mo...|[nhtsa, finds, wa...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[2.71602960309987...|[-81.077103984971...|[0.98289517774586...|       0.0|\n",
      "|40479342|       0|          apache.org|    false|2024-05-26 02:19:57| KAFKA 14180  Hel...|[KAFKA-14180] Hel...|    Samsa Needs Help|https://issues.ap...|   twitchard|    1|[, kafka, 14180, ...|[, kafka, 14180, ...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[0.39310954781708...|[-28.593351193468...|[0.73020595637693...|       0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "========= 2024-05-26 14:47:30 =========\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     aid|comments|       domain|frontpage|          posted_at|         source_text|source_title|               title|                 url|user|votes|              tokens|               words|         tf_features|      tfidf_features|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|40479363|       0|superuser.run|    false|2024-05-26 02:24:13|Superuser    Supe...|   Superuser|Use Touch ID to e...|https://superuser...|dvrp|    1|[superuser, , , ,...|[superuser, , , ,...|(10,[0,1,2,3,4,5,...|(10,[0,1,2,3,4,5,...|[0.14294892647894...|[-13.557066446148...|[0.53758251979407...|       0.0|\n",
      "+--------+--------+-------------+---------+-------------------+--------------------+------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
